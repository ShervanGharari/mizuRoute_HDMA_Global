{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# section 1 load all the necessary modules and packages\n",
    "import glob\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import netCDF4 as nc4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from shapely.geometry import Polygon\n",
    "# not neccessary for the function but for visualziation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-positive",
   "metadata": {},
   "source": [
    "# Subseting large lakes based on desired characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the hydrolakes shapefile\n",
    "# read hydrolakes\n",
    "shp = gpd.read_file('/Volumes/F:/hydrography/hydrolakes/HydroLAKES_polys_v10_shp/HydroLAKES_polys_v10_shp/HydroLAKES_polys_v10.shp')\n",
    "# subset to a minimume area\n",
    "shp_sub = shp [shp['Lake_area']>10] # select the lakes with area more than 100 km2\n",
    "# save this to subset to a shapefile\n",
    "shp_sub.to_file('/Volumes/F:/hydrography/test/hydrolake_sub10km.shp') # save the subset of the shapefile as a new shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-confidentiality",
   "metadata": {},
   "source": [
    "# Interseting the lakes and HDMA river network\n",
    "\n",
    "## using the v.overlay in Qgis; the first vector is HDMA and the second is hydro_lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the intersected shapefile; the output of v.overlay in Qgis between the subset lakes and HDMA rivers\n",
    "shp = gpd.read_file('/Volumes/F:/hydrography/test/river_intersect_lake_10.shp')\n",
    "# initialize the lake flage, lake id and parameter fields\n",
    "shp['islake'] = 0 # lake flags are initialized as zeros\n",
    "shp['lakeId'] = 0 # lake ids are initialized as zeros\n",
    "shp['RATEACV'] = 0 # lake ids are initialized as zeros\n",
    "shp['RATEBCV'] = 0 # lake ids are initialized as zeros\n",
    "# loop over the element of the shapefile\n",
    "print(len(shp))\n",
    "for index, row in shp.iterrows():\n",
    "    print(index)\n",
    "    lake_id = row['b_Hylak_id'] # get the id for lake for the river segment\n",
    "    shp_sub = shp[shp['b_Hylak_id'] == lake_id] # subset the river segment inside a lake\n",
    "    max_area = max(shp_sub['a_flow_acc']) # get the maximume contributing area of that lake assuming that is the outflow\n",
    "    if row['a_flow_acc'] == max_area: # if equal to max_area then that segment is a lake\n",
    "        shp['islake'].iloc[index] = 1\n",
    "        shp['lakeId'].iloc[index] = row['b_Hylak_id']\n",
    "        shp['RATEACV'].iloc[index] = 0.01 /(3600*24) # 0.01 day-1 to per second-1; should be corrected\n",
    "        shp['RATEBCV'].iloc[index] = 1.5 # set to 1 for now\n",
    "    if row['a_Tosegmen'] == -9999: # if the down sream id is -9999 then that segment is also a lake but a closed lake\n",
    "        shp['islake'].iloc[index] = 1\n",
    "        shp['lakeId'].iloc[index] = row['b_Hylak_id']\n",
    "        shp['RATEACV'].iloc[index] = 0.0 # the lake does not drain outward\n",
    "        shp['RATEBCV'].iloc[index] = 1.5 # set to one for now\n",
    "shp.to_file('/Volumes/F:/hydrography/test/river_intersect_lake_10_lake_flag.shp') # save to the file\n",
    "print(shp.shape) # size of the shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-glass",
   "metadata": {},
   "source": [
    "## In the process of v.overlay there might be many simiar segments of shapefile created that have the same lake id and the same river segment id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data frame based on the \n",
    "dataframe = shp.drop(['fid', 'cat', 'a_cat', 'a_BotElev', 'a_Length', 'a_OBJECTID', 'a_PFAF',\n",
    "       'a_PFAF_COD', 'a_PF_TYPE', 'a_Shape_Le', 'a_Slope', 'a_TopElev',\n",
    "       'a_Tosegmen', 'a_end_x', 'a_end_y', 'a_flow_acc',\n",
    "       'a_start_x', 'a_start_y', 'b_cat', 'b_Hylak_id', 'b_Lake_nam',\n",
    "       'b_Country', 'b_Continen', 'b_Poly_src', 'b_Lake_typ', 'b_Grand_id',\n",
    "       'b_Lake_are', 'b_Shore_le', 'b_Shore_de', 'b_Vol_res',\n",
    "       'b_Vol_src', 'b_Depth_av', 'b_Dis_avg', 'b_Res_time', 'b_Elevatio',\n",
    "       'b_Slope_10', 'b_Wshd_are', 'b_Pour_lon', 'b_Pour_lat', 'geometry'], axis=1); # keeping the 'b_Vol_tota'\n",
    "\n",
    "dataframe['seg_id']= dataframe['a_seg_id'] # rename seg_id\n",
    "dataframe = dataframe.drop_duplicates() # remove similar rows in the shapefile\n",
    "dataframe = dataframe.reset_index(drop=True) # reindex the dataframe\n",
    "print(dataframe.shape) # size of the shapefile, compare with pervious block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-cambodia",
   "metadata": {},
   "source": [
    "## Removing the elements that are not flagges as lake as they may not have the highest contributing area (not identified as outlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non lake elements from the dat frame\n",
    "dataframe = dataframe[dataframe['islake']!=0]\n",
    "dataframe = dataframe.reset_index(drop=True) # reindexing of the shriked shapefile\n",
    "print(dataframe.shape) # size of the shapefile, compare with pervious block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-national",
   "metadata": {},
   "source": [
    "## Next is to read the entire river network again and assign the lake flag to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the entire streamflow segments, creat the new fields and then loop \n",
    "shp = gpd.read_file('/Users/shg096/Desktop/processed/hdma_global_stream.shp')\n",
    "\n",
    "# initialize the lake model simulation\n",
    "shp['islake'] = 0 # lake model simulation\n",
    "shp['lakeId'] = 0 # lake id\n",
    "shp['RATEACV'] = 0 # lake vol\n",
    "shp['RATEBCV'] = 0\n",
    "shp['lake_Vol'] = 0\n",
    "\n",
    "for i in np.arange(dataframe.shape[0]): # the sieze of dataframe from the last block\n",
    "    #print(i)\n",
    "    idx = shp[shp['seg_id'] == dataframe['seg_id'].iloc[i]].index.to_numpy() # find the location of the lake in river segments\n",
    "    if 1 <idx.shape[0]: # check there is no dublication\n",
    "        print(idx)\n",
    "    shp ['islake'].iloc[idx] = 1\n",
    "    shp ['lakeId'].iloc[idx] = dataframe['lakeId'].iloc[i]\n",
    "    shp ['RATEACV'].iloc[idx] = dataframe['RATEACV'].iloc[i]\n",
    "    shp ['RATEBCV'].iloc[idx] = dataframe['RATEBCV'].iloc[i]\n",
    "    shp ['lake_Vol'].iloc[idx] = dataframe['b_Vol_tota'].iloc[i]\n",
    "    print(shp ['lake_Vol'].iloc[idx])\n",
    "\n",
    "shp.to_file('/Volumes/F:/hydrography/test/river_with_lake_flag4_10km.shp')\n",
    "print(shp.shape) # check the shape\n",
    "print(sum(shp['islake'])) # check the number of lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-samoa",
   "metadata": {},
   "source": [
    "## Re-ordering for the HDMA reorder setup for mizuRoute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_1 = shp[shp['seg_id']<2900000]\n",
    "temp_1 = temp_1[temp_1['seg_id']>1900000]\n",
    "#print(temp_1)\n",
    "\n",
    "temp_2 = shp[shp['seg_id']<6900000]\n",
    "temp_2 = temp_2[temp_2['seg_id']>5900000]\n",
    "#print(temp_2)\n",
    "\n",
    "temp_3 = shp[shp['seg_id']<1900000]\n",
    "temp_3 = temp_3[temp_3['seg_id']>900000]\n",
    "#print(temp_3)\n",
    "\n",
    "temp_4 = shp[shp['seg_id']<4900000]\n",
    "temp_4 = temp_4[temp_4['seg_id']>3900000]\n",
    "#print(temp_4)\n",
    "\n",
    "temp_5 = shp[shp['seg_id']<3900000]\n",
    "temp_5 = temp_5[temp_5['seg_id']>2900000]\n",
    "#print(temp_5)\n",
    "\n",
    "temp_6 = shp[shp['seg_id']<5900000]\n",
    "temp_6 = temp_6[temp_6['seg_id']>4900000]\n",
    "#print(temp_6)\n",
    "\n",
    "rdf = gpd.GeoDataFrame( pd.concat( [temp_1, temp_2, temp_3, temp_4, temp_5, temp_6], ignore_index=True) )\n",
    "rdf.to_file('/Volumes/F:/hydrography/test/river_with_lake_flag4_10km_reorder.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-trauma",
   "metadata": {},
   "source": [
    "## Check the reordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = gpd.read_file('/Volumes/F:/hydrography/test/river_with_lake_flag4_10km_reorder.shp')\n",
    "print(shp.columns)\n",
    "#plt.plot(shp['islake'])\n",
    "print(sum(shp['islake']))\n",
    "plt.plot(shp['seg_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-posting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
